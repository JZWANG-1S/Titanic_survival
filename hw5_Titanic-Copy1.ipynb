{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "dataset_joined = [train, test]\n",
    "dataset_joined = pd.concat(dataset_joined)\n",
    "print(dataset_joined.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features we'd like to talk about:\n",
    "\n",
    "### Age:\n",
    "According to the in-class lab, it's obvious that age plays an important role in predicting if a person survived. However there're nulls in the age column. We'd like to fill them with random numbers generated by a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with sex encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder_sex = LabelEncoder()\n",
    "dataset_joined['Sex'] = labelencoder_sex.fit_transform(dataset_joined['Sex'])\n",
    "\n",
    "# fill age\n",
    "median_0 = dataset_joined[(dataset_joined['Sex'] == 0)]['Age'].dropna().median()\n",
    "var_0 = dataset_joined[ (dataset_joined['Sex'] == 0)]['Age'].dropna().std()\n",
    "median_1 = dataset_joined[(dataset_joined['Sex'] == 1)]['Age'].dropna().median()\n",
    "var_1 = dataset_joined[ (dataset_joined['Sex'] == 1)]['Age'].dropna().std()\n",
    "dataset_joined.loc[(dataset_joined['Sex'] == 0),'Age'] = dataset_joined[(dataset_joined['Sex'] == 0)]['Age'].fillna(np.random.normal(median_0, var_0))\n",
    "dataset_joined.loc[(dataset_joined['Sex'] == 1),'Age'] = dataset_joined[(dataset_joined['Sex'] == 1)]['Age'].fillna(np.random.normal(median_1, var_1))\n",
    "dataset_joined.info()\n",
    "print('\\n Now age is filled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pclass:\n",
    "Pictures from the in-class lab shows that there's a relationship between Pclass and chance of survival. We need to turn the Pclass feature into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_joined = pd.get_dummies(dataset_joined, columns=['Pclass'], drop_first=True)\n",
    "dataset_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles: \n",
    "One couldn't have failed to notice the titles in the passengers' names. We simply divided them into three catagories: 'Mr', 'Miss' and others. Again we need to turn them into dummy variables to avoid dummy variable trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_joined['Name'] = dataset_joined['Name'].str.split(',').str[1]\n",
    "dataset_joined['Name'] = dataset_joined['Name'].str.split('.').str[0]   \n",
    "names = dataset_joined['Name'].copy()\n",
    "for item in names:\n",
    "    if (item == ' Mr'):\n",
    "        names.replace(item, 1, inplace = True)\n",
    "    elif (item == ' Miss' or item == ' Mrs'):\n",
    "        names.replace(item, 0, inplace = True)\n",
    "    elif (item == ' Capt' or item == ' Col' or item == ' Don' or item == ' Dona' or item == ' Dr' or item == ' Jonkheer' or item == ' Lady' or item == ' Major' or item == ' Master' or item == ' Mile' or item == ' Mlle' or item == ' Mme' or item == ' Ms' or item == ' Rev' or item == ' Sir' or item == ' the Countess'):\n",
    "        names.replace(item, 2, inplace = True)\n",
    "    \n",
    "dataset_joined['Name'] = names\n",
    "dataset_joined = pd.get_dummies(dataset_joined, columns=['Name'], drop_first=True)\n",
    "dataset_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alone\n",
    "According to the lab, whether a person is alone means a lot to the chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_joined['Alone'] = (dataset_joined['SibSp'] + dataset_joined['Parch'])>0\n",
    "dataset_joined['Alone'] = dataset_joined['Alone'].map({True: 1, False: 0})\n",
    "dataset_joined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with other features and cleanse, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked, perform dummy variable\n",
    "dataset_joined['Embarked'].fillna('S', inplace=True)\n",
    "labelencoder = LabelEncoder()\n",
    "dataset_joined['Embarked'] = labelencoder.fit_transform(dataset_joined['Embarked'])\n",
    "dataset_joined = pd.get_dummies(dataset_joined, columns=['Embarked'], drop_first=True)\n",
    "# fare feature\n",
    "dataset_joined['Fare'].fillna(dataset_joined['Fare'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into Train and Test\n",
    "dataset_train = dataset_joined.iloc[:891, :]\n",
    "dataset_test = dataset_joined.iloc[891:, :]\n",
    "\n",
    "# Splitting the dataset into the input and output\n",
    "dataset_train.info()\n",
    "x_train = dataset_train.iloc[:,[0,2,3,5,6,9,10,11,12,13,14,15]]\n",
    "y_train = dataset_train.iloc[:, [7]]\n",
    "x_test = dataset_test.iloc[:,[0,2,3,5,6,9,10,11,12,13,14,15]]\n",
    "print('\\nx_train head is:')\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train = StandardScaler().fit_transform(x_train)\n",
    "x_test = StandardScaler().fit_transform(x_test)\n",
    "y_train = y_train.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features we chose to be in the model are: Age, fare, parch, sex, sibsp, pclass, name, alone, embarked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def models():\n",
    "    return [LogisticRegression(),\n",
    "            SVC(),\n",
    "            LinearSVC(),\n",
    "            RandomForestClassifier(),\n",
    "            KNeighborsClassifier(),\n",
    "            GaussianNB(),\n",
    "            DecisionTreeClassifier(),\n",
    "            MLPClassifier(alpha=0.01, hidden_layer_sizes=(15), max_iter=10000),\n",
    "            XGBClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(model):\n",
    "    return model.__class__.__name__\n",
    "\n",
    "\n",
    "def best_model(model2predict):\n",
    "    score_df = series_best_first(model2predict)\n",
    "    print(score_df)\n",
    "    return score_df.keys()[0]\n",
    "\n",
    "\n",
    "def series_best_first(model2predict):\n",
    "    model2score = {k: v[1] for k, v in model2predict.items()}\n",
    "    return pd.Series(model2score).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def write_submission(predict, submission_csv):\n",
    "    submission = pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": predict\n",
    "    })\n",
    "    submission.to_csv(submission_csv, index=False)\n",
    "    return pd.read_csv(submission_csv).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection based on the `score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_score(model, inputs):\n",
    "    X, y, X_test = inputs\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred, model.score(X, y)\n",
    "\n",
    "inputs = x_train, y_train.ravel(), x_test\n",
    "table = {name(m): predict_and_score(m, inputs) for m in models()}\n",
    "model = best_model(table)\n",
    "print(table.keys())\n",
    "write_submission(table[model][0].astype(np.int8), 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection based on the `cross_val_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def predict_and_cv_score(model, inputs):\n",
    "    X, y, X_test = inputs\n",
    "    score = cross_val_score(model, X, y, cv=10, scoring='accuracy').mean()\n",
    "    model.fit(X, y)\n",
    "    return model.predict(X_test), score\n",
    "\n",
    "\n",
    "table = {name(m): predict_and_cv_score(m, inputs) for m in models()}\n",
    "model = best_model(table)\n",
    "write_submission(table[model][0].astype(np.int8), 'submission_sv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "## Here we achieved our best score: 0.80861."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve prediction by voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2predict = {name(m): predict_and_score(m, inputs) for m in models()}\n",
    "series = series_best_first(model2predict)\n",
    "print(series)\n",
    "best_models = series.keys()[0:5].tolist()\n",
    "print(best_models)\n",
    "ys = [v[0] for k, v in model2predict.items() if k in best_models]\n",
    "print(len(ys))\n",
    "sum_ys = np.sum(ys, axis=0)\n",
    "# print(sum_ys)\n",
    "votes = sum_ys // 3\n",
    "# print(votes)\n",
    "\n",
    "write_submission(votes.astype(np.int8), 'submission_vote.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Improve prediction by parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = {m:predict_and_score(SVC(C=m*0.1), inputs) for m in range(1,100)}\n",
    "table2 = {m:predict_and_cv_score(SVC(C=m*0.1), inputs) for m in range(1,100)}\n",
    "\n",
    "# model1 = best_model(table1)\n",
    "write_submission(table1[best_model(table1)][0].astype(np.int8), 'submission_sv1.csv')\n",
    "\n",
    "# model2 = best_model(table2)\n",
    "write_submission(table2[best_model(table2)][0].astype(np.int8), 'submission_sv2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We let the parameter C of SVC model to be in the range(0.1,10) and evaluate them with score and cross-val-score. Turns out that the best result we can achieve is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to one of our best submissions:\n",
    "https://www.kaggle.com/submissions/6746327/6746327.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
